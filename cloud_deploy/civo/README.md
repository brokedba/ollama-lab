# üöÄ LLM Inference Deployment on Civo Cloud Talos K8s Cluster

## **Overview**
This repository provides an end-to-end solution for deploying a **local LLM (Large Language Model) inference** setup on a **Civo Cloud Talos Kubernetes Cluster**. 

The deployment includes:

- ‚úÖ **Civo Kubernetes Cluster (Talos)**
- ‚úÖ **Ollama LLM Server**
- ‚úÖ **Open WebUI for model interaction**
- ‚úÖ **Traefik as the Ingress Controller**
- ‚úÖ **Cert-Manager for TLS (Self-Signed Certificate)**
- ‚úÖ **Okta Authentication for Secure Access** (Optional)

---

## **1Ô∏è‚É£ Prerequisites**
Before you begin, ensure you have the following:

- üõ† [Terraform](https://developer.hashicorp.com/terraform/downloads) (`>=1.5`)
- üõ† [kubectl](https://kubernetes.io/docs/tasks/tools/) (`>=1.25`)
- üõ† [Helm](https://helm.sh/docs/intro/install/) (`>=3.10`)
- ‚òÅ **Civo Cloud Account** with an API key
- üîë **Okta Developer Account** (if enabling okta authentication)
- üíª **Local or Cloud-Based Machine** to run the deployment

---

## **2Ô∏è‚É£ Setup Civo Kubernetes Cluster (Talos)**

### Use an env-vars File**
Export your **TF_VARS** i.e **Civo API Key** in the env-vars file :
```bash
export CIVO_TOKEN="your-civo-api-key"
export TF_VAR_region="NYC1"
#### [OKTA] Optional ##################
export TF_VAR_enable_okta="true"
export TF_VAR_okta_client_id="your-okta-client-id"
export TF_VAR_okta_client_secret="your-okta-client-secret"
export TF_VAR_okta_openid_provider="https://your-okta-domain/oauth2/default"
```
- Load the Variables into Your Shell Before running Terraform, source the env-vars file:
```bash
$ source env-vars
```
  
## **3Ô∏è‚É£ Run Terraform Now that the variables are set, run Terraform:**
```
terraform plan
terraform apply
```
# 4Ô∏è‚É£ Destroying the Infrastructure
To delete everything:
```
terraform destroy -auto-approve
```
# 5Ô∏è‚É£ Next Steps
- Add GPU Acceleration for LLM inference using nvidia drivers plugin  
- Implement Persistent Storage for Models like S3
- Optimize Autoscaling for Traffic Spikes

----
